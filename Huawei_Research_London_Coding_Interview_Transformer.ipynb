{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QinyanGong/2024RL/blob/main/Huawei_Research_London_Coding_Interview_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lYSpjpSoN4KP"
      },
      "source": [
        "# Coding Test\n",
        "\n",
        "You will be assessed overall on:\n",
        "\n",
        "    1) How far you get in the alloted time.\n",
        "    2) Code optimisations.\n",
        "    3) Code reusability.\n",
        "    4) Code readability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KEJtvPiiN4KS"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC5Q_agrN4KS"
      },
      "source": [
        "## 1. Coding Test\n",
        "Below you will find some code which is a classical example of the use of a transformer model.\n",
        "Your task is to complete the code such that the training runs and the validation loss decreases.\n",
        "\n",
        "Based on [Attention is all you need, Vaswani et al. 2017](https://arxiv.org/abs/1706.03762), write the code\n",
        "1. for the multi-head self-attention mechanism\n",
        "2. for the forward pass of the layer\n",
        "3. for generating causal masks.\n",
        "\n",
        "Finally, run the training and validation with the provided functions and dataset.\n",
        "\n",
        "Note: `pip install portalocker`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install portalocker\n",
        "! pip install torchtext==0.12.0\n",
        "! pip install torchdata==0.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_si07duOiDq",
        "outputId": "5f3ae695-7120-4cef-cae9-e346e9e1f887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torchtext==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (2.32.3)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchtext==0.12.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2024.8.30)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torchdata==0.3.0\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl.metadata (970 bytes)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchdata==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (2024.8.30)\n",
            "Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.4.0\n",
            "    Uninstalling torchdata-0.4.0:\n",
            "      Successfully uninstalled torchdata-0.4.0\n",
            "Successfully installed torchdata-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yd0HGFPxweS",
        "outputId": "7c688873-2b55-4f0c-f7d5-daa184430dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWyTnfg15xj",
        "outputId": "eaeb1714-fe16-4813-c65e-ae171c751f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "v8ySf6SwN4KT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import math\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import PennTreebank\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AnQxSbP5N4KU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, n_heads: int, dropout: Optional[float] = None):\n",
        "        \"\"\"\n",
        "        NOTE:   There are different ways of implementing the self-attention.\n",
        "                In this exercise we assume that the linear projection for the key, query and value\n",
        "                is defined by a single matrix of shape (dim, dim).\n",
        "                In your implementation, make the necessary operations such that each head is of equal\n",
        "                dimension `dim / n_heads`.\n",
        "                Also make sure that your implementation is compatible with a mask.\n",
        "\n",
        "        ==================================================\n",
        "        Creates a multi-head self-attention block\n",
        "        ==================================================\n",
        "\n",
        "        :param dim: total dimension of the model (embedding dimension for the input).\n",
        "        :param n_heads: number of heads in the multi-head attention.\n",
        "        :param dropout: dropout probability, to avoid overfitting.\n",
        "        \"\"\"\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        # Each attention head will have a dimension of `dim / n_heads`.\n",
        "        self.head_dim = dim // n_heads\n",
        "        assert n_heads * self.head_dim == dim, f\"embedding dim={dim} not divisible by n_heads={n_heads}.\"\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear_query = nn.Linear(dim, dim)\n",
        "        self.linear_key = nn.Linear(dim, dim)\n",
        "        self.linear_value = nn.Linear(dim, dim)\n",
        "        self.linear_cat_attn = nn.Linear(dim, dim)\n",
        "\n",
        "    def attention(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        ==================================================\n",
        "        compute scaled dot-product attention\n",
        "        ==================================================\n",
        "\n",
        "        :param query: given sentence that we focused on\n",
        "        :param key: every sentence to check relationship with qeury\n",
        "        :param value: every sentence same with Key to get the value\n",
        "        :param mask: mask for the attention\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # key is a 4 dimension tensor\n",
        "        # [batch_size, head, length, dim_tensor]\n",
        "        batch_size, head, length, dim_tensor = key.shape\n",
        "\n",
        "        # 1. dot product query with key^T\n",
        "        key_transpose = key.transpose(-2,-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_tensor)\n",
        "\n",
        "        # 2. apply masking (optional)\n",
        "        # if mask is not None:\n",
        "        #   scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        # 3. softmax\n",
        "        scores = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # 4. multiply with value\n",
        "        scores = scores @ value\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        NOTE: the forward pass calls self.attention()\n",
        "\n",
        "        ==================================================\n",
        "        compute multi-head self-attention\n",
        "        ==================================================\n",
        "\n",
        "        :param q: given sentence that we focused on\n",
        "        :param k: every sentence to check relationship with qeury\n",
        "        :param v: every sentence same with Key to get the value\n",
        "        :param mask: mask for the attention\n",
        "        :return: multihead self-attention output\n",
        "        \"\"\"\n",
        "        # 1. dot product with weight matrices to get q,k,v\n",
        "        q = self.linear_query(q)\n",
        "        k = self.linear_key(k)\n",
        "        v = self.linear_value(v)\n",
        "\n",
        "        #  2. split tensor by number of heads [batch_size, length, dim]->[batch_size, head, length, d_tensor]\n",
        "        split = lambda x: x.view(x.shape[0], x.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        q = split(q)\n",
        "        k = split(k)\n",
        "        v = split(v)\n",
        "\n",
        "        # 3. compute attention\n",
        "        attn = self.attention(q, k, v, mask)\n",
        "\n",
        "        # 4. convert back to [batch_size, length, dim]\n",
        "        output = attn.transpose(1, 2).contiguous().view(q.size(0), -1, self.dim)\n",
        "        output = self.linear_cat_attn(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test for tensor.view\n",
        "x = torch.randint(0,100,(1,2,12))\n",
        "print(x)\n",
        "# [batch_size, length, dim]->[batch_size, length, head,  d_tensor]\n",
        "print(x.view(x.shape[0], x.shape[1], 3, 4))\n",
        "# [batch_size, length, head,  d_tensor]->[batch_size, head, length, d_tensor]\n",
        "print(x.view(x.shape[0], x.shape[1], 3, 4).transpose(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phtLyCzoWodh",
        "outputId": "09746c44-15fd-430b-dc3e-64e7735d7430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[73, 68,  5, 56, 81, 35, 93, 50, 69, 65, 92, 91],\n",
            "         [94, 68, 75, 62, 76, 14, 23, 45, 90, 98, 75, 80]]])\n",
            "tensor([[[[73, 68,  5, 56],\n",
            "          [81, 35, 93, 50],\n",
            "          [69, 65, 92, 91]],\n",
            "\n",
            "         [[94, 68, 75, 62],\n",
            "          [76, 14, 23, 45],\n",
            "          [90, 98, 75, 80]]]])\n",
            "tensor([[[[73, 68,  5, 56],\n",
            "          [94, 68, 75, 62]],\n",
            "\n",
            "         [[81, 35, 93, 50],\n",
            "          [76, 14, 23, 45]],\n",
            "\n",
            "         [[69, 65, 92, 91],\n",
            "          [90, 98, 75, 80]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "PB4sAGJSN4KV"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, attn: MultiHeadSelfAttention, d_model: int, dim_feedforward: int, layer_norm_eps=1e-5, dropout=0.1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        # Multi-head self-attention\n",
        "        self.attn = attn\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "        # Dropout Layer\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Activation\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        ==================================================\n",
        "        create forward pass for the encoder block:\n",
        "        1. self-attention\n",
        "        2. add & norm\n",
        "        3. feed forward\n",
        "        4. add & norm\n",
        "        ==================================================\n",
        "\n",
        "        :param x: input tensor\n",
        "        :param mask: mask for the attention\n",
        "        :return: output encoded tensor\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. self-attention\n",
        "        attn_output = self._self_attn(x, mask)\n",
        "\n",
        "        # 2. add residual & layer normalization\n",
        "        x = x + attn_output\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # 3. feed forward\n",
        "        ff_output = self._feed_forward(x)\n",
        "\n",
        "        # 4. add residual & layer normalization\n",
        "        x = x + ff_output\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _self_attn(self, x: Tensor, mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.attn(q=x, k=x, v=x, mask=mask)\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    def _feed_forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(self.activation(self.linear1(x)))\n",
        "        return self.dropout2(self.linear2(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "9CWsqGPVN4KV"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder: EncoderBlock, n_blocks: int):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([deepcopy(encoder) for _ in range(n_blocks)])\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "E2j-qk_lN4KW"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "    \"\"\"\n",
        "    ==================================================\n",
        "    create mask for causal attention, where each token in sequence is predicted one by one,\n",
        "    and the model shouldn't have access to future tokens during the prediction process.\n",
        "    ==================================================\n",
        "\n",
        "    :param size: size of the mask\n",
        "    :return: mask\n",
        "    \"\"\"\n",
        "    # print(size)\n",
        "    # creates an upper triangular matrix of ones with shape (size,size), and transpose\n",
        "\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1). transpose(0, 1)\n",
        "    # mask with -inf\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "3vUAGKYhN4KW"
      },
      "outputs": [],
      "source": [
        "class TransformerModelManualAttn(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        # Positional Encoding layer\n",
        "        self.pos_encoder = PositionalEncodingTorch(d_model, dropout)\n",
        "        # Encoder Block\n",
        "        encoder_layers = EncoderBlock(attn=MultiHeadSelfAttention(dim=emsize, n_heads=nhead, dropout=dropout),\n",
        "                                      d_model=d_model, dim_feedforward=d_hid, dropout=dropout)\n",
        "        # Transformer Encoder with nlayers layers\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        # Final Linear layer to map output to vocabulary size\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        ==================================================\n",
        "        Forward pass of the transformer encoder\n",
        "        ==================================================\n",
        "\n",
        "        :param src: input tensor\n",
        "        :param src_mask: mask for the attention\n",
        "        :return: output tensor\n",
        "        \"\"\"\n",
        "        # token embedding + positional encoding\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RUOBTV5dN4KX"
      },
      "source": [
        "The code below is given as-is, please contact the examinator if there is any issue running this, unrelated to your code above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "vNG0Uq81N4KX"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodingTorch(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "xNnc2-TtN4KX"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset, vocab, tokenizer) -> Tensor:\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "\n",
        "def batchify(data: Tensor, bsz: int, device: torch.device = None) -> Tensor:\n",
        "    seq_len = data.shape[0] // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: Tensor, i: int, bptt: int) -> Tuple[Tensor, Tensor]:\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "zC8Mipc1N4KX"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "        model,\n",
        "        train_data: Tensor,\n",
        "        bptt: int,\n",
        "        criterion,\n",
        "        ntokens: int,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        scheduler: torch.optim.lr_scheduler,\n",
        "        epoch: int = 0,\n",
        "        device: torch.device = None,\n",
        "        use_causal_mask: bool = True\n",
        ") -> None:\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device) if use_causal_mask else None\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.shape[0] - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt=bptt)\n",
        "        if data.shape[0] < bptt and src_mask is not None:\n",
        "            src_mask = generate_square_subsequent_mask(data.shape[0]).to(device)\n",
        "        src_mask = src_mask[:min(data.shape[0],data.shape[1]), :min(data.shape[0],data.shape[1])]\n",
        "\n",
        "        # print(data.shape)\n",
        "        # print(src_mask.shape)\n",
        "        output = model(src=data, src_mask=src_mask)\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f}'\n",
        "                  f' | ppl {ppl:8.2f}'\n",
        "                  )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "        model,\n",
        "        eval_data: Tensor,\n",
        "        bptt: int,\n",
        "        ntokens: int,\n",
        "        criterion,\n",
        "        device: torch.device = None,\n",
        "        use_causal_mask: bool = True,\n",
        ") -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device) if use_causal_mask else None\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.shape[0] - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i, bptt=bptt)\n",
        "            if data.shape[0] < bptt and src_mask is not None:\n",
        "                src_mask = generate_square_subsequent_mask(data.shape[0]).to(device)\n",
        "            output = model(data, src_mask=src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += data.shape[0] * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK651WYXN4KY",
        "outputId": "2a8ec74f-be3f-46b5-f535-817afa456e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch  8.15 | loss  7.76 | ppl  2347.95\n",
            "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch  7.75 | loss  6.89 | ppl   981.64\n",
            "| epoch   1 |   600/ 1320 batches | lr 5.00 | ms/batch  8.17 | loss  6.73 | ppl   837.84\n",
            "| epoch   1 |   800/ 1320 batches | lr 5.00 | ms/batch  8.71 | loss  6.66 | ppl   777.16\n",
            "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch  7.78 | loss  6.62 | ppl   750.70\n",
            "| epoch   1 |  1200/ 1320 batches | lr 5.00 | ms/batch  7.82 | loss  6.61 | ppl   744.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 11.01s | valid loss  6.64 | valid ppl   768.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1320 batches | lr 4.75 | ms/batch  7.86 | loss  6.63 | ppl   755.74\n",
            "| epoch   2 |   400/ 1320 batches | lr 4.75 | ms/batch  7.83 | loss  6.62 | ppl   750.81\n",
            "| epoch   2 |   600/ 1320 batches | lr 4.75 | ms/batch  7.87 | loss  6.62 | ppl   748.68\n",
            "| epoch   2 |   800/ 1320 batches | lr 4.75 | ms/batch  8.37 | loss  6.59 | ppl   730.22\n",
            "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch  8.77 | loss  6.58 | ppl   717.77\n",
            "| epoch   2 |  1200/ 1320 batches | lr 4.75 | ms/batch  8.02 | loss  6.52 | ppl   677.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 11.10s | valid loss  6.69 | valid ppl   802.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1320 batches | lr 4.51 | ms/batch  7.97 | loss  6.53 | ppl   687.85\n",
            "| epoch   3 |   400/ 1320 batches | lr 4.51 | ms/batch  7.91 | loss  6.49 | ppl   659.01\n",
            "| epoch   3 |   600/ 1320 batches | lr 4.51 | ms/batch  7.95 | loss  6.40 | ppl   601.12\n",
            "| epoch   3 |   800/ 1320 batches | lr 4.51 | ms/batch  7.88 | loss  6.36 | ppl   577.60\n",
            "| epoch   3 |  1000/ 1320 batches | lr 4.51 | ms/batch  8.79 | loss  6.26 | ppl   520.93\n",
            "| epoch   3 |  1200/ 1320 batches | lr 4.51 | ms/batch  8.36 | loss  6.18 | ppl   482.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 11.12s | valid loss  6.24 | valid ppl   512.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1320 batches | lr 4.29 | ms/batch  7.90 | loss  6.10 | ppl   446.41\n",
            "| epoch   4 |   400/ 1320 batches | lr 4.29 | ms/batch  7.85 | loss  6.04 | ppl   420.33\n",
            "| epoch   4 |   600/ 1320 batches | lr 4.29 | ms/batch  7.80 | loss  5.99 | ppl   398.58\n",
            "| epoch   4 |   800/ 1320 batches | lr 4.29 | ms/batch  7.87 | loss  5.90 | ppl   365.61\n",
            "| epoch   4 |  1000/ 1320 batches | lr 4.29 | ms/batch  8.06 | loss  5.86 | ppl   350.45\n",
            "| epoch   4 |  1200/ 1320 batches | lr 4.29 | ms/batch  8.75 | loss  5.79 | ppl   326.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 11.01s | valid loss  5.86 | valid ppl   350.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1320 batches | lr 4.07 | ms/batch  7.81 | loss  5.76 | ppl   315.80\n",
            "| epoch   5 |   400/ 1320 batches | lr 4.07 | ms/batch  7.81 | loss  5.73 | ppl   308.44\n",
            "| epoch   5 |   600/ 1320 batches | lr 4.07 | ms/batch  7.72 | loss  5.72 | ppl   306.04\n",
            "| epoch   5 |   800/ 1320 batches | lr 4.07 | ms/batch  7.85 | loss  5.66 | ppl   286.28\n",
            "| epoch   5 |  1000/ 1320 batches | lr 4.07 | ms/batch  7.75 | loss  5.64 | ppl   281.03\n",
            "| epoch   5 |  1200/ 1320 batches | lr 4.07 | ms/batch  8.07 | loss  5.58 | ppl   264.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 10.99s | valid loss  5.74 | valid ppl   311.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1320 batches | lr 3.87 | ms/batch  7.80 | loss  5.58 | ppl   265.44\n",
            "| epoch   6 |   400/ 1320 batches | lr 3.87 | ms/batch  7.76 | loss  5.57 | ppl   262.24\n",
            "| epoch   6 |   600/ 1320 batches | lr 3.87 | ms/batch  7.71 | loss  5.57 | ppl   261.86\n",
            "| epoch   6 |   800/ 1320 batches | lr 3.87 | ms/batch  7.74 | loss  5.52 | ppl   248.60\n",
            "| epoch   6 |  1000/ 1320 batches | lr 3.87 | ms/batch  7.71 | loss  5.51 | ppl   246.49\n",
            "| epoch   6 |  1200/ 1320 batches | lr 3.87 | ms/batch  7.71 | loss  5.45 | ppl   232.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 10.75s | valid loss  5.65 | valid ppl   283.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1320 batches | lr 3.68 | ms/batch  8.74 | loss  5.45 | ppl   233.63\n",
            "| epoch   7 |   400/ 1320 batches | lr 3.68 | ms/batch  7.70 | loss  5.45 | ppl   233.33\n",
            "| epoch   7 |   600/ 1320 batches | lr 3.68 | ms/batch  7.70 | loss  5.44 | ppl   231.57\n",
            "| epoch   7 |   800/ 1320 batches | lr 3.68 | ms/batch  7.70 | loss  5.40 | ppl   222.18\n",
            "| epoch   7 |  1000/ 1320 batches | lr 3.68 | ms/batch  7.71 | loss  5.40 | ppl   222.46\n",
            "| epoch   7 |  1200/ 1320 batches | lr 3.68 | ms/batch  7.69 | loss  5.35 | ppl   210.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 10.78s | valid loss  5.58 | valid ppl   264.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1320 batches | lr 3.49 | ms/batch  8.26 | loss  5.36 | ppl   213.66\n",
            "| epoch   8 |   400/ 1320 batches | lr 3.49 | ms/batch  8.90 | loss  5.36 | ppl   212.67\n",
            "| epoch   8 |   600/ 1320 batches | lr 3.49 | ms/batch  7.75 | loss  5.36 | ppl   212.17\n",
            "| epoch   8 |   800/ 1320 batches | lr 3.49 | ms/batch  7.73 | loss  5.31 | ppl   202.72\n",
            "| epoch   8 |  1000/ 1320 batches | lr 3.49 | ms/batch  7.70 | loss  5.33 | ppl   205.67\n",
            "| epoch   8 |  1200/ 1320 batches | lr 3.49 | ms/batch  7.73 | loss  5.26 | ppl   192.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 10.94s | valid loss  5.58 | valid ppl   266.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1320 batches | lr 3.32 | ms/batch  9.05 | loss  5.29 | ppl   197.76\n",
            "| epoch   9 |   400/ 1320 batches | lr 3.32 | ms/batch  9.46 | loss  5.28 | ppl   197.16\n",
            "| epoch   9 |   600/ 1320 batches | lr 3.32 | ms/batch  8.33 | loss  5.28 | ppl   195.99\n",
            "| epoch   9 |   800/ 1320 batches | lr 3.32 | ms/batch  7.77 | loss  5.24 | ppl   188.52\n",
            "| epoch   9 |  1000/ 1320 batches | lr 3.32 | ms/batch  7.72 | loss  5.26 | ppl   192.19\n",
            "| epoch   9 |  1200/ 1320 batches | lr 3.32 | ms/batch  7.77 | loss  5.19 | ppl   180.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 11.35s | valid loss  5.58 | valid ppl   264.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1320 batches | lr 3.15 | ms/batch  7.80 | loss  5.22 | ppl   185.04\n",
            "| epoch  10 |   400/ 1320 batches | lr 3.15 | ms/batch  7.81 | loss  5.22 | ppl   185.81\n",
            "| epoch  10 |   600/ 1320 batches | lr 3.15 | ms/batch  8.72 | loss  5.22 | ppl   184.73\n",
            "| epoch  10 |   800/ 1320 batches | lr 3.15 | ms/batch  8.10 | loss  5.18 | ppl   178.15\n",
            "| epoch  10 |  1000/ 1320 batches | lr 3.15 | ms/batch  7.84 | loss  5.20 | ppl   181.29\n",
            "| epoch  10 |  1200/ 1320 batches | lr 3.15 | ms/batch  7.79 | loss  5.13 | ppl   169.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 10.95s | valid loss  5.48 | valid ppl   240.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.43 | test ppl   228.41\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "## Load and batch data\n",
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data = data_process(train_iter, vocab=vocab, tokenizer=tokenizer)\n",
        "val_data = data_process(val_iter, vocab=vocab, tokenizer=tokenizer)\n",
        "test_data = data_process(test_iter, vocab=vocab, tokenizer=tokenizer)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bptt = 35\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size, device=device)\n",
        "val_data = batchify(val_data, eval_batch_size, device=device)\n",
        "test_data = batchify(test_data, eval_batch_size, device=device)\n",
        "\n",
        "# Model Parameters\n",
        "ntokens = len(vocab)\n",
        "emsize = 200\n",
        "d_hid = 200\n",
        "nlayers = 2\n",
        "nhead = 2\n",
        "dropout = 0.1\n",
        "use_causal_mask = True\n",
        "\n",
        "# Create Model\n",
        "model = TransformerModelManualAttn(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "\n",
        "### Run model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, )\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "# Train\n",
        "home = os.path.join(os.path.expanduser(\"~\"), \"transformer_test\")\n",
        "save_dir = os.path.join(os.path.join(home, 'pytorch_example')) # feel free to change path\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_model_params_path = os.path.join(save_dir, \"best_model_params.pt\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_data, bptt, criterion, ntokens, optimizer, scheduler, epoch, device, use_causal_mask)\n",
        "    val_loss = evaluate(model, val_data, bptt, ntokens, criterion, device, use_causal_mask)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    scheduler.step()\n",
        "model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
        "\n",
        "# Test\n",
        "test_loss = evaluate(model, test_data, bptt, ntokens, criterion, device)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MGFYL4kqN4KY"
      },
      "source": [
        "## 2. General knowledge and LLMs\n",
        "\n",
        "Describe the attention complexity in memory and computation costs. Do you know methods to try to reduce this cost?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wHsqsRGnN4KY"
      },
      "source": [
        "### Memory Complexity\n",
        "\n",
        "Considering a sequence of length N, for each computation, queries, keys and values are calculated with weights of size $(N * dim)$, where dim is the dimension of each vector.\n",
        "\n",
        "Therefore, the memory complexity is of $O(N^2 * dim)$, i.e. $O(N^2)$, which grows quadratically with sequence length\n",
        "\n",
        "### Computation Complexity\n",
        "\n",
        "queries, keys, values, and scores are computed from dot products, so the coputation complexity is of the order $O(N^2 * dim)$, i.e. $O(N^2)$.\n",
        "\n",
        "### Low cost attentions\n",
        "\n",
        "Sparse Attention\n",
        "Top-k Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vHRrzOcHN4KY"
      },
      "source": [
        "Why use a causal mask in attention? Are there any other interesting masks or patterns that can be used for training or for generation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "l5wAcXsON4KY"
      },
      "source": [
        "Causal mask can force model predict the future without seeing it. Then the model will be trained to predict the next token while avoid 'cheating' by looking at the future tokens.\n",
        "\n",
        "Other interesting masks include bidirectional mask, like BERT model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}